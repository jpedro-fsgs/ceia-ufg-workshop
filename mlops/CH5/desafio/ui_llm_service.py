"""
Desafio CH5 ‚Äî Interface de Chat para o Servi√ßo LLM

Objetivo: construir uma UI de chat com Streamlit que consome
a API do servi√ßo LLM rodando no Cloud Run.

Siga os coment√°rios marcados com TODO para implementar cada parte.
N√£o existe uma √∫nica forma certa ‚Äî use a estrutura abaixo como guia.

Depend√™ncias:
    pip install streamlit requests

Como rodar:
    export API_URL=https://seu-servico-xxxx-uc.a.run.app
    streamlit run ui_llm_service.py
"""

import os
import json
import time

import requests
import streamlit as st


# ------------------------------------------------------------
# CONFIGURA√á√ÉO DA P√ÅGINA
# Dica: st.set_page_config() deve ser a primeira chamada Streamlit
# ------------------------------------------------------------

st.set_page_config(page_title="Chat LLM Service", page_icon="ü§ñ")


# ------------------------------------------------------------
# T√çTULO E DESCRI√á√ÉO
# ------------------------------------------------------------

st.title("ü§ñ Chat LLM Service")
st.write("Converse com o servi√ßo LLM publicado no Cloud Run.")


# ------------------------------------------------------------
# URL DA API
# Dica: nunca cole a URL diretamente no c√≥digo.
#       Leia de uma vari√°vel de ambiente com os.getenv().
#       Defina um valor padr√£o para facilitar testes locais.
# ------------------------------------------------------------

API_URL = os.getenv("API_URL", "http://localhost:8000").rstrip("/")


# ------------------------------------------------------------
# HIST√ìRICO DE MENSAGENS
# Para que o chat "lembre" das mensagens anteriores durante
# a sess√£o, voc√™ precisa armazen√°-las em st.session_state.
#
# Estrutura sugerida para cada mensagem:
#   {"role": "user" | "assistant", "content": "texto aqui"}
#
# Dica: inicialize a lista apenas se ela ainda n√£o existir.
# ------------------------------------------------------------

if "messages" not in st.session_state:
    st.session_state["messages"] = []


# ------------------------------------------------------------
# EXIBI√á√ÉO DO HIST√ìRICO
# Renderize as mensagens j√° existentes na tela.
# Dica: st.chat_message(role) cria o bal√£o correto para cada papel.
# ------------------------------------------------------------

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])


# ------------------------------------------------------------
# FUN√á√ÉO DE CHAMADA √Ä API
# Encapsule a l√≥gica HTTP em uma fun√ß√£o separada.
# Isso facilita testes e deixa o c√≥digo mais organizado.
#
# Assinatura sugerida:
#   def call_llm(messages: list[dict]) -> str
#
# O que ela deve fazer:
#   1. Montar o payload no formato que a API espera
#      (consulte /docs do seu servi√ßo para ver o schema)
#   2. Fazer um POST para {API_URL}/chat
#   3. Extrair e retornar apenas o texto da resposta
#   4. Em caso de erro, retornar uma mensagem amig√°vel
#      (n√£o deixe o erro estourar na tela do usu√°rio)
#
# Dica: use requests.post() com o par√¢metro json= para o payload
# Dica: inspecione response.json() para ver o que a API retorna
# ------------------------------------------------------------

def stream_llm(messages: list[dict]):
    payload = {
        "messages": messages,
        "model": "gpt-4o-mini",
    }

    try:
        with requests.post(
            f"{API_URL}/chat/stream",
            json=payload,
            headers={"Accept": "text/event-stream"},
            stream=True,
            timeout=(10, 300),
        ) as response:
            response.raise_for_status()

            event_data_lines = []
            for raw_line in response.iter_lines(decode_unicode=True):
                if raw_line is None:
                    continue

                line = raw_line.strip("\r")

                if line == "":
                    if not event_data_lines:
                        continue

                    raw_event = "\n".join(event_data_lines)
                    event_data_lines = []

                    try:
                        event = json.loads(raw_event)
                    except json.JSONDecodeError:
                        continue

                    event_type = event.get("type")
                    if event_type == "token":
                        content = event.get("content", "")
                        if content:
                            yield content
                    elif event_type == "error":
                        yield f"\n\n‚ö†Ô∏è {event.get('content', 'Erro ao processar resposta do modelo.')}"
                        return
                    elif event_type == "done":
                        return

                    continue

                if line.startswith("data:"):
                    event_data_lines.append(line[5:].lstrip())

            if event_data_lines:
                try:
                    event = json.loads("\n".join(event_data_lines))
                    if event.get("type") == "token":
                        content = event.get("content", "")
                        if content:
                            yield content
                except json.JSONDecodeError:
                    pass
    except requests.RequestException:
        yield (
            "N√£o consegui conectar ao servi√ßo agora. "
            "Verifique a vari√°vel API_URL e tente novamente."
        )


# ------------------------------------------------------------
# CAIXA DE ENTRADA DO USU√ÅRIO
# Dica: st.chat_input() fica fixo na parte inferior da tela
#       e retorna o texto digitado (ou None se vazio).
# ------------------------------------------------------------

user_input = st.chat_input("Digite sua mensagem...")

# Quando o usu√°rio enviar uma mensagem, voc√™ deve:
#   1. Adicion√°-la ao hist√≥rico (role: "user")
#   2. Exibi-la na tela imediatamente
#   3. Chamar a fun√ß√£o call_llm com o hist√≥rico completo
#   4. Adicionar a resposta ao hist√≥rico (role: "assistant")
#   5. Exibir a resposta na tela

if user_input and user_input.strip():
    user_message = {"role": "user", "content": user_input.strip()}
    st.session_state["messages"].append(user_message)

    with st.chat_message("user"):
        st.markdown(user_message["content"])

    with st.chat_message("assistant"):
        placeholder = st.empty()
        assistant_text = ""
        last_render = 0.0
        for chunk in stream_llm(st.session_state["messages"]):
            assistant_text += chunk

            now = time.perf_counter()
            should_render = (now - last_render) >= 0.05 or chunk.endswith(
                ("\n", ".", "!", "?", ":")
            )
            if should_render:
                placeholder.markdown(f"{assistant_text}‚ñå")
                last_render = now

        if not assistant_text:
            assistant_text = "N√£o recebi resposta do servi√ßo. Tente novamente."
        placeholder.markdown(assistant_text)

    st.session_state["messages"].append(
        {"role": "assistant", "content": assistant_text}
    )


# ------------------------------------------------------------
# DICAS FINAIS
#
# - st.spinner("...") exibe um indicador de carregamento
#   enquanto a API responde ‚Äî melhora muito a experi√™ncia
#
# - st.error("...") exibe mensagens de erro em vermelho
#
# - Se quiser limpar o hist√≥rico, st.button("Nova conversa")
#   combinado com del st.session_state["messages"] funciona bem
#
# - Explore st.sidebar para colocar configura√ß√µes (ex: URL da API,
#   temperatura do modelo) fora da √°rea principal do chat
# ------------------------------------------------------------
